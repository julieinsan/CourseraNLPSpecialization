{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Naive Bayes\n",
    "\n",
    "The first part if this is on your CVP computer.\n",
    "\n",
    "\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "It's easier to predict the weather, given that it is California and winter,than just predicting the weather, without any other information.\n",
    "\n",
    "The conditional probability that a tweet is positive given that it has the word \"happy\" in it is equal to the intersection of the tweets that are positive and the tweets that have the word happy in it, divided by the probability that the tweet has the word \"happy\" in it.\n",
    "\n",
    "$$ P(Positive | ``happy\") = \\frac{P(Positive \\cap ``happy\")}{P(``happy\")} $$\n",
    "\n",
    "![](vennConditionalProb.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could write the Probability of a tweet having the word \"happy\" in it given that the tweet is positive as:\n",
    "\n",
    "$$ P(``happy\" | Positive) = \\frac{P(``happy\" \\cap Positive)}{P(Postitive)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to write the Probability that a tweet positive given that it has the word \"happy\" in it in terms of the Probability that the tweet has the word happy in it, given that it is Positive, we could take advantage of the fact that $ P(``happy\" \\cap Positive) $ is the same as $ P(Positive \\cap ``happy\") $ then \n",
    "\n",
    "$$ P(Positive | ``happy\") = P(``happy\" | Positive) * \\frac{P(Positive)}{P(``happy\")} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **Bayes Rule**:\n",
    "\n",
    "$$ P(X|Y) = P(Y|X) * \\frac{P(X)}{P(Y)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes For Sentiment Analysis: Introduction\n",
    "\n",
    "It's called Naive because it assumes that the features that you use for classification are all independent, which, in reality is rarely the case. To build a classifier, we first start by creating the conditional probabilities of each word in the vocabulary:\n",
    "\n",
    "![](CondProbNaiveBayes.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dividing the number of instances over the total number of words in each class, you get the following table of probabilities:\n",
    "\n",
    "![](condProbs.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some words appear about equally in both classes (e.g. I, am, learning, NLP).  These don't have much predictive power.  Other words only appear in one class but not the other.  These can't be used.  happy, sad and not are the most influential.\n",
    "\n",
    "Given the above probabilities, we can computer the **likelihood score** of a particular tweet as follows:\n",
    "\n",
    "![](likelihoodScore.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score > 1 indicates the class is positive, else negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Smoothing\n",
    "\n",
    "Use Laplacian Smoothing to avoid Probability of zero in a particular class (like *because* above). If a word does not appear in training, it automatically gets a probability of zero.  To fix this, use Laplacian Smoothing.\n",
    "\n",
    "**V**: the number of unique words in your vocabulary\n",
    "\n",
    "To calculate the probability of word i is in class:\n",
    "\n",
    "$$ P(w_{i} | class) = \\frac{freq(w_{i}, class)}{N_{class}}  $$\n",
    "\n",
    "class $ \\in $ { Positive, Negative}\n",
    "$N_{class}$ = frequency of all words in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian smoothing adds 1 to the numerator and the number of all words in the entire vocabulary (Positive and Negative tweets) in the denominator.\n",
    "\n",
    "$$ P(w_{i} | class) = \\frac{freq(w_{i}, class) + 1}{N_{class} + V}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the new $ P(because | Negative) $ is 0.05 instead of 0, which makes the likelihood score computable again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood, Part 1\n",
    "\n",
    "The conditional probability ratios for word_i are:\n",
    "\n",
    "$$ ratio(w_{i}) = \\frac{P(w_{i} | Pos)}{P(w_{i} | Neg)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ratio is > 1 for positive words, 1 for neutral and < 1 for negative words.\n",
    "\n",
    "For all words in a tweet, the product of the ratios above for each word is called the **likelihood** that the tweet is positive.  If you multiply by the Probability that the tweet is positive divided by the probability that the tweet is negative, or the **prior ratio**, you get the full **Naive Bayes** equation.  If the sample is balanced, then the prior ratio is 1.\n",
    "\n",
    "$$ \\frac{P(pos)}{P(neg)} \\prod_{i=1}\\frac{P(w_{i} | Pos)}{P(w_{i} | Neg)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood\n",
    "\n",
    "Take the log of the likelihood so you aren't multiplying smaller and smaller numbers and getting underflow from computer.\n",
    "\n",
    "$$ log( a * b) = log(a) + log(b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(\\frac{P(pos)}{P(neg)} \\prod_{i=1}\\frac{P(w_{i} | Pos)}{P(w_{i} | Neg)}) \\Rightarrow  log\\frac{P(pos)}{P(neg)} + \\sum_{i=1} log \\frac{P(w_{i} | Pos)}{P(w_{i} | Neg)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the first term on the right is the **log prior** and the second term is the **log likelihood**.\n",
    "\n",
    "So we calculate the lambda for each word in tweet:\n",
    "\n",
    "$$ \\lambda(w) = \\frac{P(w| Pos)}{P(w | Neg)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lambda.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if our tweet is: <scan style=\"color: lightgreen\">I am happy because I am learning</scan> then we can calculate the loglikelihood by summing up the $\\lambda$ s for each word in the tweet:\n",
    "\n",
    "log likelihood = 0 + 0 + 2.2 + 0 + 0 + 0 + 1.1 = 3.3\n",
    "\n",
    "A value less than zero would indicate that the tweet was negative, zero is neutral, positive values are positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b13f566935e67f52ffef42a9a86540e892131f9b826c20df54b22e1c2e60ae77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
