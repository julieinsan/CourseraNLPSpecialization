{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Week 3: Vector Spaces\n","\n","There are two different designs:\n","\n","* Word by Word design: vector represents a certain word. Then you make a matrix where a row is the number of times in the document that the word shows up near each of the other words, with a window size of k.\n","\n","* Word by Document design: if you have many labelled documents in a corpus, you count how many times a word appears in the documents in a particular label.  If you want to compare words, you would put them on the x and y axes and have a vector representation of each label. You can use the angle between 2 vectors to measure similarity.\n","\n","## Euclidean Distance\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","a = np.array([1,0,-1,6,8])\n","b = np.array([0, 11, 4, 7, 6])\n","d = np.linalg.norm(a-b)\n","d"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Cosine Similarity\n","\n","The cosine of the angle between 2 vectors. It's a better measure of similarity because it doesn't depend on the relative sizes of the corpuses."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["You can use known relationships to make predictions about unknown relationships.\n","\n","Ex. I know the capital of US is Washington, DC, what is the capital of Russia?\n","\n","![](CosineSimilarity.PNG)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","word_embeddings = pickle.load( open( \"word_embeddings_subset.p\", \"rb\" ) )\n","\n","word_embeddings.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(word_embeddings['country'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_embeddings['country'][0:3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max(word_embeddings['country'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["min(word_embeddings['country'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(word_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'happy' in word_embeddings.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Get the vector for a given word:\n","def vec(w):\n","    return word_embeddings[w]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Operating on word embeddings\n","\n","Remember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\n","\n","Word embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation. \n","\n","In this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python. \n","\n","In the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector's alignment as well."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt # Import matplotlib\n","%matplotlib inline\n","\n","words = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n","\n","bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n","\n","fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n","\n","col1 = 3 # Select the column for the x axis\n","col2 = 2 # Select the column for the y axis\n","\n","# Print an arrow for each word\n","for word in bag2d:\n","    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n","\n","    \n","ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n","\n","# Add the word label over each dot in the scatter plot\n","for i in range(0, len(words)):\n","    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n","\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Note that similar words like 'village' and 'town' or 'petroleum', 'oil', and 'gas' tend to point in the same direction. Also, note that 'sad' and 'happy' look close to each other; however, the vectors point in opposite directions.\n","\n","In this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics.\n","\n","## Word distance\n","\n","Now plot the words 'sad', 'happy', 'town', and 'village'. In this same chart, display the vector from 'village' to 'town' and the vector from 'sad' to 'happy'. Let us use NumPy for these linear algebra operations."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["words = ['sad', 'happy', 'town', 'village']\n","\n","bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n","\n","fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n","\n","col1 = 3 # Select the column for the x axe\n","col2 = 2 # Select the column for the y axe\n","\n","# Print an arrow for each word\n","for word in bag2d:\n","    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n","    \n","# print the vector difference between village and town\n","village = vec('village')\n","town = vec('town')\n","diff = town - village\n","ax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n","\n","# print the vector difference between village and town\n","sad = vec('sad')\n","happy = vec('happy')\n","diff = happy - sad\n","ax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n","\n","\n","ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n","\n","# Add the word label over each dot in the scatter plot\n","for i in range(0, len(words)):\n","    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n","\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bag2d"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Linear algebra on word embeddings\n","\n","In the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\n","\n","To start, get the **norm** of a word in the word embedding."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(np.linalg.norm(vec('town'))) # Print the norm of the word town\n","print(np.linalg.norm(vec('sad'))) # Print the norm of the word sad"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Predicting capitals\n","\n","Now, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between 'France' and 'Paris' represents the concept of Capital.\n","\n","One can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["capital = vec('France') - vec('Paris')\n","country = vec('Madrid') + capital\n","\n","print(country[0:5]) # Print the first 5 values of the vector"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can observe that the vector 'country' that we expected to be the same as the vector for Spain is not exactly it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["diff = country - vec('Spain')\n","print(diff[0:10])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["So, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be 'Spain'. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\n","keys = word_embeddings.keys()\n","data = []\n","for key in keys:\n","    data.append(word_embeddings[key])\n","\n","embedding = pd.DataFrame(data=data, index=keys)\n","# Define a function to find the closest word to a vector:\n","def find_closest_word(v, k = 1):\n","    # Calculate the vector difference from each word to the input vector\n","    diff = embedding.values - v \n","    # Get the norm of each difference vector. \n","    # It means the squared euclidean distance from each word to the input vector\n","    delta = np.sum(diff * diff, axis=1)\n","    # Find the index of the minimun distance in the array\n","    i = np.argmin(delta)\n","    # Return the row name for this item\n","    return embedding.iloc[i].name"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>290</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>country</th>\n","      <td>-0.080078</td>\n","      <td>0.133789</td>\n","      <td>0.143555</td>\n","      <td>0.094727</td>\n","      <td>-0.047363</td>\n","      <td>-0.023560</td>\n","      <td>-0.008545</td>\n","      <td>-0.186523</td>\n","      <td>0.045898</td>\n","      <td>-0.081543</td>\n","      <td>...</td>\n","      <td>-0.145508</td>\n","      <td>0.067383</td>\n","      <td>-0.244141</td>\n","      <td>-0.077148</td>\n","      <td>0.047607</td>\n","      <td>-0.075195</td>\n","      <td>-0.149414</td>\n","      <td>-0.044189</td>\n","      <td>0.097168</td>\n","      <td>0.067383</td>\n","    </tr>\n","    <tr>\n","      <th>city</th>\n","      <td>-0.010071</td>\n","      <td>0.057373</td>\n","      <td>0.183594</td>\n","      <td>-0.040039</td>\n","      <td>-0.029785</td>\n","      <td>-0.079102</td>\n","      <td>0.071777</td>\n","      <td>0.013306</td>\n","      <td>-0.143555</td>\n","      <td>0.011292</td>\n","      <td>...</td>\n","      <td>0.024292</td>\n","      <td>-0.168945</td>\n","      <td>-0.062988</td>\n","      <td>0.117188</td>\n","      <td>-0.020508</td>\n","      <td>0.030273</td>\n","      <td>-0.247070</td>\n","      <td>-0.122559</td>\n","      <td>0.076172</td>\n","      <td>-0.234375</td>\n","    </tr>\n","    <tr>\n","      <th>China</th>\n","      <td>-0.073242</td>\n","      <td>0.135742</td>\n","      <td>0.108887</td>\n","      <td>0.083008</td>\n","      <td>-0.127930</td>\n","      <td>-0.227539</td>\n","      <td>0.151367</td>\n","      <td>-0.045654</td>\n","      <td>-0.065430</td>\n","      <td>0.034424</td>\n","      <td>...</td>\n","      <td>0.140625</td>\n","      <td>0.087402</td>\n","      <td>0.152344</td>\n","      <td>0.079590</td>\n","      <td>0.006348</td>\n","      <td>-0.037842</td>\n","      <td>-0.183594</td>\n","      <td>0.137695</td>\n","      <td>0.093750</td>\n","      <td>-0.079590</td>\n","    </tr>\n","    <tr>\n","      <th>Iraq</th>\n","      <td>0.191406</td>\n","      <td>0.125000</td>\n","      <td>-0.065430</td>\n","      <td>0.060059</td>\n","      <td>-0.285156</td>\n","      <td>-0.102539</td>\n","      <td>0.117188</td>\n","      <td>-0.351562</td>\n","      <td>-0.095215</td>\n","      <td>0.200195</td>\n","      <td>...</td>\n","      <td>-0.100586</td>\n","      <td>-0.077148</td>\n","      <td>-0.123047</td>\n","      <td>0.193359</td>\n","      <td>-0.153320</td>\n","      <td>0.089355</td>\n","      <td>-0.173828</td>\n","      <td>-0.054688</td>\n","      <td>0.302734</td>\n","      <td>0.105957</td>\n","    </tr>\n","    <tr>\n","      <th>oil</th>\n","      <td>-0.139648</td>\n","      <td>0.062256</td>\n","      <td>-0.279297</td>\n","      <td>0.063965</td>\n","      <td>0.044434</td>\n","      <td>-0.154297</td>\n","      <td>-0.184570</td>\n","      <td>-0.498047</td>\n","      <td>0.047363</td>\n","      <td>0.110840</td>\n","      <td>...</td>\n","      <td>-0.195312</td>\n","      <td>-0.345703</td>\n","      <td>0.217773</td>\n","      <td>-0.091797</td>\n","      <td>0.051025</td>\n","      <td>0.061279</td>\n","      <td>0.194336</td>\n","      <td>0.204102</td>\n","      <td>0.235352</td>\n","      <td>-0.051025</td>\n","    </tr>\n","    <tr>\n","      <th>town</th>\n","      <td>0.123535</td>\n","      <td>0.159180</td>\n","      <td>0.030029</td>\n","      <td>-0.161133</td>\n","      <td>0.015625</td>\n","      <td>0.111816</td>\n","      <td>0.039795</td>\n","      <td>-0.196289</td>\n","      <td>-0.039307</td>\n","      <td>0.067871</td>\n","      <td>...</td>\n","      <td>-0.007935</td>\n","      <td>-0.091797</td>\n","      <td>-0.265625</td>\n","      <td>0.029297</td>\n","      <td>0.089844</td>\n","      <td>-0.049805</td>\n","      <td>-0.202148</td>\n","      <td>-0.079590</td>\n","      <td>0.068848</td>\n","      <td>-0.164062</td>\n","    </tr>\n","    <tr>\n","      <th>Canada</th>\n","      <td>-0.136719</td>\n","      <td>-0.154297</td>\n","      <td>0.269531</td>\n","      <td>0.273438</td>\n","      <td>0.086914</td>\n","      <td>-0.076172</td>\n","      <td>-0.018677</td>\n","      <td>0.006256</td>\n","      <td>0.077637</td>\n","      <td>-0.211914</td>\n","      <td>...</td>\n","      <td>0.105469</td>\n","      <td>0.030762</td>\n","      <td>-0.039307</td>\n","      <td>0.183594</td>\n","      <td>-0.117676</td>\n","      <td>0.191406</td>\n","      <td>0.074219</td>\n","      <td>0.020996</td>\n","      <td>0.285156</td>\n","      <td>-0.257812</td>\n","    </tr>\n","    <tr>\n","      <th>London</th>\n","      <td>-0.267578</td>\n","      <td>0.092773</td>\n","      <td>-0.238281</td>\n","      <td>0.115234</td>\n","      <td>-0.006836</td>\n","      <td>0.221680</td>\n","      <td>-0.251953</td>\n","      <td>-0.055420</td>\n","      <td>0.020020</td>\n","      <td>0.149414</td>\n","      <td>...</td>\n","      <td>-0.008667</td>\n","      <td>-0.008484</td>\n","      <td>-0.053223</td>\n","      <td>0.197266</td>\n","      <td>-0.296875</td>\n","      <td>0.064453</td>\n","      <td>0.091797</td>\n","      <td>0.058350</td>\n","      <td>0.022583</td>\n","      <td>-0.101074</td>\n","    </tr>\n","    <tr>\n","      <th>England</th>\n","      <td>-0.198242</td>\n","      <td>0.115234</td>\n","      <td>0.062500</td>\n","      <td>-0.058350</td>\n","      <td>0.226562</td>\n","      <td>0.045898</td>\n","      <td>-0.062256</td>\n","      <td>-0.202148</td>\n","      <td>0.080566</td>\n","      <td>0.021606</td>\n","      <td>...</td>\n","      <td>0.135742</td>\n","      <td>0.109375</td>\n","      <td>-0.121582</td>\n","      <td>0.008545</td>\n","      <td>-0.171875</td>\n","      <td>0.086914</td>\n","      <td>0.070312</td>\n","      <td>0.003281</td>\n","      <td>0.069336</td>\n","      <td>0.056152</td>\n","    </tr>\n","    <tr>\n","      <th>Australia</th>\n","      <td>0.048828</td>\n","      <td>-0.194336</td>\n","      <td>-0.041504</td>\n","      <td>0.084473</td>\n","      <td>-0.114258</td>\n","      <td>-0.208008</td>\n","      <td>-0.164062</td>\n","      <td>-0.269531</td>\n","      <td>0.079102</td>\n","      <td>0.275391</td>\n","      <td>...</td>\n","      <td>0.021118</td>\n","      <td>0.171875</td>\n","      <td>0.042236</td>\n","      <td>0.221680</td>\n","      <td>-0.239258</td>\n","      <td>-0.106934</td>\n","      <td>0.030884</td>\n","      <td>0.006622</td>\n","      <td>0.051270</td>\n","      <td>-0.135742</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10 rows Ã— 300 columns</p>\n","</div>"],"text/plain":["                0         1         2         3         4         5    \\\n","country   -0.080078  0.133789  0.143555  0.094727 -0.047363 -0.023560   \n","city      -0.010071  0.057373  0.183594 -0.040039 -0.029785 -0.079102   \n","China     -0.073242  0.135742  0.108887  0.083008 -0.127930 -0.227539   \n","Iraq       0.191406  0.125000 -0.065430  0.060059 -0.285156 -0.102539   \n","oil       -0.139648  0.062256 -0.279297  0.063965  0.044434 -0.154297   \n","town       0.123535  0.159180  0.030029 -0.161133  0.015625  0.111816   \n","Canada    -0.136719 -0.154297  0.269531  0.273438  0.086914 -0.076172   \n","London    -0.267578  0.092773 -0.238281  0.115234 -0.006836  0.221680   \n","England   -0.198242  0.115234  0.062500 -0.058350  0.226562  0.045898   \n","Australia  0.048828 -0.194336 -0.041504  0.084473 -0.114258 -0.208008   \n","\n","                6         7         8         9    ...       290       291  \\\n","country   -0.008545 -0.186523  0.045898 -0.081543  ... -0.145508  0.067383   \n","city       0.071777  0.013306 -0.143555  0.011292  ...  0.024292 -0.168945   \n","China      0.151367 -0.045654 -0.065430  0.034424  ...  0.140625  0.087402   \n","Iraq       0.117188 -0.351562 -0.095215  0.200195  ... -0.100586 -0.077148   \n","oil       -0.184570 -0.498047  0.047363  0.110840  ... -0.195312 -0.345703   \n","town       0.039795 -0.196289 -0.039307  0.067871  ... -0.007935 -0.091797   \n","Canada    -0.018677  0.006256  0.077637 -0.211914  ...  0.105469  0.030762   \n","London    -0.251953 -0.055420  0.020020  0.149414  ... -0.008667 -0.008484   \n","England   -0.062256 -0.202148  0.080566  0.021606  ...  0.135742  0.109375   \n","Australia -0.164062 -0.269531  0.079102  0.275391  ...  0.021118  0.171875   \n","\n","                292       293       294       295       296       297  \\\n","country   -0.244141 -0.077148  0.047607 -0.075195 -0.149414 -0.044189   \n","city      -0.062988  0.117188 -0.020508  0.030273 -0.247070 -0.122559   \n","China      0.152344  0.079590  0.006348 -0.037842 -0.183594  0.137695   \n","Iraq      -0.123047  0.193359 -0.153320  0.089355 -0.173828 -0.054688   \n","oil        0.217773 -0.091797  0.051025  0.061279  0.194336  0.204102   \n","town      -0.265625  0.029297  0.089844 -0.049805 -0.202148 -0.079590   \n","Canada    -0.039307  0.183594 -0.117676  0.191406  0.074219  0.020996   \n","London    -0.053223  0.197266 -0.296875  0.064453  0.091797  0.058350   \n","England   -0.121582  0.008545 -0.171875  0.086914  0.070312  0.003281   \n","Australia  0.042236  0.221680 -0.239258 -0.106934  0.030884  0.006622   \n","\n","                298       299  \n","country    0.097168  0.067383  \n","city       0.076172 -0.234375  \n","China      0.093750 -0.079590  \n","Iraq       0.302734  0.105957  \n","oil        0.235352 -0.051025  \n","town       0.068848 -0.164062  \n","Canada     0.285156 -0.257812  \n","London     0.022583 -0.101074  \n","England    0.069336  0.056152  \n","Australia  0.051270 -0.135742  \n","\n","[10 rows x 300 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Print some rows of the embedding as a Dataframe\n","embedding.head(10)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["'Spain'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["find_closest_word(country)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Predicting other Countries"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["'Spain'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["find_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Germany\n"]}],"source":["print(find_closest_word(vec('Berlin') + capital))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Germany\n"]}],"source":["print(find_closest_word(vec('Italy') - vec('Rome') + vec('Berlin')))"]},{"cell_type":"markdown","metadata":{},"source":["## Represent a sentence as a vector\n","\n","A whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see. "]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["array([ 2.87475586e-02,  1.03759766e-01,  1.32629395e-01,  3.33007812e-01,\n","       -2.61230469e-02, -5.95703125e-01, -1.25976562e-01, -1.01306152e+00,\n","       -2.18544006e-01,  6.60705566e-01, -2.58300781e-01, -2.09960938e-02,\n","       -7.71484375e-02, -3.07128906e-01, -5.94726562e-01,  2.00561523e-01,\n","       -1.04980469e-02, -1.10748291e-01,  4.82177734e-02,  6.38977051e-01,\n","        2.36083984e-01, -2.69775391e-01,  3.90625000e-02,  4.16503906e-01,\n","        2.83416748e-01, -7.25097656e-02, -3.12988281e-01,  1.05712891e-01,\n","        3.22265625e-02,  2.38403320e-01,  3.88183594e-01, -7.51953125e-02,\n","       -1.26281738e-01,  6.60644531e-01, -7.89794922e-01, -7.04345703e-02,\n","       -1.14379883e-01, -4.78515625e-02,  4.76318359e-01,  5.31127930e-01,\n","        8.10546875e-02, -1.17553711e-01,  1.02050781e+00,  5.59814453e-01,\n","       -1.17187500e-01,  1.21826172e-01, -5.51574707e-01,  1.44531250e-01,\n","       -7.66113281e-01,  5.36102295e-01, -2.80029297e-01,  3.85986328e-01,\n","       -2.39135742e-01, -2.86865234e-02, -5.10498047e-01,  2.59658813e-01,\n","       -7.52929688e-01,  4.32128906e-02, -7.17773438e-02, -1.26708984e-01,\n","        4.40673828e-02,  5.12939453e-01, -5.15808105e-01,  1.20117188e-01,\n","       -5.52978516e-02, -3.92089844e-01, -3.15917969e-01,  1.57226562e-01,\n","       -3.19702148e-01,  1.75170898e-01, -3.81835938e-01, -2.07031250e-01,\n","       -4.72717285e-02, -2.79296875e-01, -3.29040527e-01, -1.69067383e-01,\n","        1.61132812e-02,  1.71569824e-01,  5.73730469e-02, -2.44140625e-03,\n","        8.34960938e-02, -1.58203125e-01, -3.10119629e-01,  5.28564453e-02,\n","        8.60595703e-02,  5.12695312e-02, -7.22900391e-01,  4.97924805e-01,\n","       -5.85937500e-03,  4.49951172e-01,  3.82446289e-01, -2.80029297e-01,\n","       -3.28125000e-01, -6.27441406e-02, -4.81933594e-01,  1.93176270e-02,\n","       -1.69326782e-01, -4.28649902e-01,  5.39062500e-01, -1.28417969e-01,\n","       -8.83789062e-02,  5.13916016e-01,  9.13085938e-02, -1.60156250e-01,\n","        6.86035156e-02, -9.74121094e-02, -3.70712280e-01, -3.27270508e-01,\n","        1.77978516e-01, -4.65332031e-01,  1.70410156e-01,  9.08203125e-02,\n","        2.76857376e-01, -1.69677734e-01,  3.27728271e-01, -3.12500000e-02,\n","       -2.20809937e-01, -3.46679688e-01,  4.67407227e-01,  5.31860352e-01,\n","       -1.30615234e-01, -2.36816406e-02, -6.56250000e-01, -5.79589844e-01,\n","       -2.05810547e-01, -3.03222656e-01,  1.94259644e-01, -7.28515625e-01,\n","       -4.92522240e-01, -5.37109375e-01, -3.47656250e-01,  1.08642578e-01,\n","       -1.41601562e-01, -2.07031250e-01,  2.52441406e-01, -7.78808594e-02,\n","       -5.02441406e-01,  1.53808594e-02,  8.64257812e-02,  2.59765625e-01,\n","        6.64062500e-02, -7.12890625e-01, -1.45751953e-01,  7.56835938e-03,\n","        4.87792969e-01,  1.39160156e-01,  1.15722656e-01,  1.28662109e-01,\n","       -4.75585938e-01,  2.21191406e-01,  3.25317383e-01,  1.06323242e-01,\n","       -6.11083984e-01, -3.59619141e-01,  6.54296875e-02, -2.41699219e-01,\n","       -6.29882812e-02, -1.62109375e-01,  4.26269531e-01, -4.38354492e-01,\n","        1.93725586e-01,  4.89562988e-01,  5.31494141e-01, -7.29370117e-02,\n","        1.77246094e-01,  9.39941406e-02,  2.92236328e-01, -2.74047852e-01,\n","        2.63366699e-02,  4.36035156e-01, -3.76953125e-01,  3.10546875e-01,\n","        4.87304688e-01, -2.43041992e-01,  1.21612549e-02, -3.80371094e-01,\n","        3.80493164e-01, -6.22436523e-01, -3.98071289e-01,  1.24206543e-01,\n","       -8.20312500e-01, -2.72583008e-01, -6.21582031e-01, -4.87060547e-01,\n","        3.06671143e-01, -2.61230469e-01,  5.12451172e-01,  5.55694580e-01,\n","        5.66894531e-01,  7.33886719e-01, -1.75781250e-01,  4.13574219e-01,\n","       -2.54272461e-01,  1.32507324e-01, -4.78515625e-01,  4.63256836e-01,\n","       -6.21948242e-02, -1.80664062e-01, -5.46386719e-01, -6.31103516e-01,\n","       -1.47949219e-01, -3.15185547e-01, -7.12890625e-02, -7.67578125e-01,\n","        3.92272949e-01, -1.97753906e-01,  2.23144531e-01, -5.07324219e-01,\n","        8.39843750e-02, -4.98657227e-02,  1.01074219e-01,  2.07885742e-01,\n","       -2.77343750e-01,  1.03027344e-01, -1.38671875e-01,  2.87353516e-01,\n","       -4.81895447e-01, -1.66748047e-01, -1.47277832e-01,  3.61633301e-01,\n","        6.38504028e-02, -6.69189453e-01,  1.95312500e-03, -7.34375000e-01,\n","       -1.28158569e-01,  9.76562500e-04, -7.08007812e-02,  3.72558594e-01,\n","        8.31176758e-01,  5.94482422e-01,  5.37109375e-02, -3.00140381e-01,\n","       -4.53857422e-01,  1.11511230e-01, -1.32812500e-01,  1.25732422e-01,\n","        3.39843750e-01, -2.48352051e-01, -1.62353516e-02, -2.84667969e-01,\n","        4.70703125e-01, -4.48242188e-01,  8.50753784e-02,  2.69042969e-01,\n","        3.98254395e-03, -3.53759766e-01, -3.90625000e-02, -3.22753906e-01,\n","       -6.90917969e-02, -4.13818359e-02,  1.35314941e-01, -8.50396156e-02,\n","        1.28417969e-01,  6.15966797e-01,  3.55957031e-01, -6.05468750e-02,\n","       -2.25463867e-01, -2.62207031e-01, -2.72949219e-01, -5.16113281e-01,\n","        1.59179688e-01,  2.74902344e-01, -7.61718750e-02, -3.41796875e-03,\n","        4.37500000e-01,  2.98583984e-01, -4.40795898e-01, -3.43261719e-01,\n","        1.73583984e-01,  3.32092285e-01, -2.12646484e-01,  5.76171875e-01,\n","        2.06787109e-01, -7.91015625e-02,  5.79695702e-02, -1.01806641e-01,\n","       -7.06787109e-01, -3.40576172e-02, -4.11865234e-01,  9.82666016e-02,\n","       -1.70410156e-01, -4.18212891e-01,  8.39233398e-01, -1.15722656e-01,\n","        1.28173828e-01, -2.07763672e-01, -4.08203125e-01, -1.77612305e-01,\n","        1.01196289e-01,  4.24072266e-01, -5.26428223e-02, -5.58593750e-01,\n","        1.12304688e-02, -1.12060547e-01, -9.42382812e-02,  2.35595703e-02,\n","       -3.92578125e-01, -7.12890625e-02,  5.69824219e-01,  9.81445312e-02],\n","      dtype=float32)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["doc = \"Spain petroleum city king\"\n","vdoc = [vec(x) for x in doc.split(\" \")]\n","doc2vec = np.sum(vdoc, axis = 0)\n","doc2vec"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["'petroleum'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["find_closest_word(doc2vec)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Principal Component Analysis and Visualization\n","\n","* PCA is used to reduce the dimensions of the data down to 2D so that the data can be plotted.\n","\n","* You first find uncorrelated features then reduce the dimensions down.\n","\n","![](dimensionalityReduction.PNG)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* How do we get uncorrelated features?\n","\n","Use **Eigenvectors**! They give the direction of uncorrelated features.\n","\n","* How do we reduce dimensions while retaining as much info as possible?\n","\n","Use **Eigenvalues**! Eigenvalues are the variance of the new features (or eigenvectors). They represent the amount of information retained by each new feature.\n","\n","You get these from the **covariance matrix** of your data.\n","\n","### Steps to compute PCA\n","\n","* Mean Normalize your data\n","\n","* Compute the covariance matrix\n","\n","* Compute Singular Value Decomposition on the covariance matrix.  This returns [U SV], where U is the matrix of eigenvectors and S is the matrix of eigenvalues.\n","\n","* You can then use the first n columns (where n is the number of dimenstions you actually want) of vector U to get your new data by multiplying X (the original data) by U[:, 0:n].\n","\n","\n","## Lab: More on PCA\n","\n"," PCA is a statistical technique invented in 1901 by Karl Pearson that uses orthogonal transformations to map a set of variables into a set of linearly uncorrelated variables called Principal Components.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here's the PCA function that stumped me from the Assignment:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_pca(X, n_components=2):\n","    \"\"\"\n","    Input:\n","        X: of dimension (m,n) where each row corresponds to a word vector\n","        n_components: Number of components you want to keep.\n","    Output:\n","        X_reduced: data transformed in 2 dims/columns + regenerated original data\n","    pass in: data as 2D NumPy array\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    # mean center the data\n","    X_demeaned = X - np.mean(X, axis=0)\n","\n","    # calculate the covariance matrix\n","    covariance_matrix = np.cov(X_demeaned, rowvar=False)\n","\n","    # calculate eigenvectors & eigenvalues of the covariance matrix\n","    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix, UPLO='L')\n","    # sort eigenvalue in increasing order (get the indices from the sort)\n","    idx_sorted = np.argsort(eigen_vals)\n","    \n","    # reverse the order so that it's from highest to lowest.\n","    idx_sorted_decreasing = np.flip(idx_sorted)\n","\n","    # sort the eigen values by idx_sorted_decreasing\n","    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n","\n","    # sort eigenvectors using the idx_sorted_decreasing indices\n","    eigen_vecs_sorted = eigen_vecs[:,idx_sorted_decreasing]\n","\n","    # select the first n eigenvectors (n is desired dimension\n","    # of rescaled data array, or n_components)\n","    eigen_vecs_subset = eigen_vecs_sorted[:,0:n_components]\n","\n","    # transform the data by multiplying the transpose of the eigenvectors with the transpose of the de-meaned data\n","    # Then take the transpose of that product.\n","    X_reduced = np.dot(eigen_vecs_subset.T, X_demeaned.T).T\n","\n","    ### END CODE HERE ###\n","\n","    return X_reduced"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
